# -*- coding: utf-8 -*-
"""Eda_titanic_database.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ftXo0rmvfJGTJGTpO-a6o82yhaMzC-M4
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np #numerical python
import seaborn as sns # library for statistical
import matplotlib.pyplot as plt
# %matplotlib inline
import math
#collecting data
titanic_data = pd.read_csv("C:\\Users\\CHAITALI JAIN\\Desktop\\database for eds\\titanic.csv")
titanic_data.head(10)

print("number of passanger in original data: " + str(len(titanic_data.index)))

titanic_data.info()

"""## Analyze Data"""

sns.countplot(x = "Survived" , data = titanic_data)
#titanic_data["Survived"].plot.hist()

#check how many female and male in surviver
sns.countplot( x = "Survived" , hue = "Sex", data = titanic_data)

sns.countplot(x = "Survived", hue = "Pclass" , data = titanic_data)

sns.countplot(x = "Survived" , hue = "Age" , data = titanic_data)

titanic_data["Age"].plot.hist()

titanic_data['Fare'].plot.hist(bins = 20 , figsize = (,5) )
#bins signify number of intervals into which data range will be divided and igsize is a tuple specifying the width and height of the figure (in inches) that will contain the histogram plot

sns.countplot(x = "SibSp" , data = titanic_data)

"""## data wrangling  - data is removing null values in dataset"""

titanic_data.isnull()

titanic_data.isnull().sum()

sns.heatmap(titanic_data.isnull(), yticklabels=False, cmap = "viridis")
#cmap is used for color coding

#replace or drop the coloumn
sns.boxplot(x = "Pclass" , y = "Age" , data= titanic_data)

titanic_data.head(5)

titanic_data.drop("Cabin", axis = 1, inplace = True)
#drop all the null values from cabin

titanic_data.dropna(inplace = True)

sns.heatmap(titanic_data.isnull(), yticklabels=False)

titanic_data.drop("Cabin", axis = 1, inplace = True)
pcl = pd.get_dummies(titanic_data["Pclass"],drop_first = True)
embark = pd.get_dummies(titanic_data["Embarked"],drop_first = True)
sex = pd.get_dummies(titanic_data["Sex"], drop_first = True)

titanic_data.head(10)

titanic_data = pd.concat([titanic_data, sex,embark,pcl],axis =1)
titanic_data.head(10)

titanic_data.drop(["Sex", "Embarked", "PassengerId", "Name", "Ticket"], axis = 1, inplace = True)
#When inplace=True:
#The operation is applied directly to the DataFrame object, and the changes are made in place.
#No new DataFrame is returned.
#The original DataFrame is modified.

titanic_data.drop("Pclass", axis =1, inplace = True)

"""## Implelement Logistic Regression
by Train and Test Data - Build the model on the train
data and predict the output on
the test data
logistic Logistic Regression()
logistic.fit(train_X,train_Y)
"""

X = titanic_data.drop("Survived",axis =1)
Y = titanic_data["Survived"] # we goona find survived people

from sklearn.model_selection import train_test_split

print("Shape of X:", X.shape)
print("Shape of Y:", Y.shape)
#The inconsistency in the shapes of X and y suggests that there are different numbers of samples between them. In your case, X has 891 samples, while y has 1000 samples.
#To resolve this issue, you need to ensure that X and y have the same number of samples, as each sample in X corresponds to a label in y. Here are some possible solutions:

#predictions = logmodel.predict(X_test)
# Drop 'Survived' (target) and 'Cabin' (non-numeric with missing)
X = titanic_data.drop(["Survived", "Cabin"], axis=1)

# Target column
y = titanic_data["Survived"]

# Impute numeric columns
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
X = imputer.fit_transform(X)

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

logmodel = LogisticRegression()
logmodel.fit(X_train, y_train)

# testing prediction
predictions = logmodel.predict(X_test)
print(predictions[:10])  # First 10 predictions

# testing accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, predictions)
print("Accuracy:", accuracy)

"""Comparsion between different model like Random Forest, SVM, KNN"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

models = {
    "Logistic Regression": LogisticRegression(),
    "Random Forest": RandomForestClassifier(),
    "SVM": SVC(),
    "KNN": KNeighborsClassifier()
}

for name, model in models.items():
    model.fit(X_train, y_train)
    score = model.score(X_test, y_test)
    print(f"{name} Accuracy: {score:.4f}")